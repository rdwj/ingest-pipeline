#!/bin/bash
# Helper script to compile and verify the pipeline

set -e

echo "üîß KubeFlow Pipeline Verification"
echo "=================================="

# Check Python version
echo "Checking Python version..."
python --version

# Check if kfp is installed
echo ""
echo "Checking KubeFlow SDK..."
if python -c "import kfp" 2>/dev/null; then
    python -c "import kfp; print(f'‚úÖ kfp version: {kfp.__version__}')"
else
    echo "‚ùå kfp not found. Installing..."
    pip install -r requirements.txt
fi

# Compile pipeline
echo ""
echo "Compiling pipeline..."
python pipeline.py

# Check if YAML was created
if [ -f "doc_ingestion_pipeline.yaml" ]; then
    SIZE=$(wc -c < doc_ingestion_pipeline.yaml)
    echo "‚úÖ Pipeline compiled successfully"
    echo "   File: doc_ingestion_pipeline.yaml"
    echo "   Size: ${SIZE} bytes"
else
    echo "‚ùå Pipeline compilation failed"
    exit 1
fi

# Validate YAML structure
echo ""
echo "Validating YAML structure..."
if python -c "import yaml; yaml.safe_load(open('doc_ingestion_pipeline.yaml'))" 2>/dev/null; then
    echo "‚úÖ YAML structure is valid"
else
    echo "‚ùå YAML structure is invalid"
    exit 1
fi

# Count pipeline components
echo ""
echo "Pipeline components:"
COMPONENT_COUNT=$(grep -c "name: " doc_ingestion_pipeline.yaml || echo "0")
echo "   Total components: ${COMPONENT_COUNT}"

# Show pipeline structure
echo ""
echo "Pipeline structure:"
grep "  name: " doc_ingestion_pipeline.yaml | sed 's/^/   - /'

echo ""
echo "‚úÖ Pipeline is ready for upload to OpenShift AI"
echo ""
echo "Next steps:"
echo "1. Open OpenShift AI console"
echo "2. Navigate to your Data Science Project"
echo "3. Go to Pipelines ‚Üí Import pipeline"
echo "4. Upload: doc_ingestion_pipeline.yaml"
echo "5. Configure parameters from example-parameters.yaml"
