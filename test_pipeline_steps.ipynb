{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion Pipeline - Step-by-Step Testing\n",
    "\n",
    "This notebook tests each pipeline step individually to help diagnose issues.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Run this in an OpenShift AI workbench with network access to:\n",
    "- MinIO S3 endpoint\n",
    "- vector-search-service\n",
    "- PostgreSQL database\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Update these values before running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "\n",
    "# S3/MinIO Configuration\n",
    "S3_ENDPOINT = \"https://minio-api-minio.apps.meshtest.llnl.gov\"\n",
    "S3_BUCKET = \"kb-documents\"\n",
    "S3_PREFIX = \"data/\"  # Include trailing slash!\n",
    "S3_ACCESS_KEY = \"minioadmin\"  # Update with actual key\n",
    "S3_SECRET_KEY = \"minioadmin\"  # Update with actual key\n",
    "\n",
    "# Local paths\n",
    "DOWNLOAD_PATH = \"/tmp/documents\"\n",
    "FILE_EXTENSIONS = [\".md\", \".txt\", \".html\"]\n",
    "\n",
    "# Service Configuration\n",
    "SERVICE_URL = \"http://vector-search-service.servicenow-ai-poc.svc.cluster.local:8000\"\n",
    "COLLECTION_NAME = \"default\"\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Database Configuration\n",
    "DB_HOST = \"postgres-pgvector.servicenow-ai-poc.svc.cluster.local\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_USER = \"raguser\"\n",
    "DB_PASSWORD = \"\"  # MUST UPDATE\n",
    "DB_NAME = \"ragdb\"\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"  S3: {S3_ENDPOINT}/{S3_BUCKET}/{S3_PREFIX}\")\n",
    "print(f\"  Service: {SERVICE_URL}\")\n",
    "print(f\"  Collection: {COLLECTION_NAME}\")\n",
    "print(f\"  Database: {DB_HOST}:{DB_PORT}/{DB_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 requests psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download from S3/MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import boto3\nfrom pathlib import Path\nfrom botocore.config import Config\nimport warnings\nwarnings.filterwarnings('ignore')  # Suppress SSL warnings\n\nprint(f\"Downloading from S3: {S3_ENDPOINT}/{S3_BUCKET}/{S3_PREFIX}\")\n\n# Configure with explicit timeouts\nconfig = Config(\n    connect_timeout=10,\n    read_timeout=30,\n    retries={'max_attempts': 3}\n)\n\n# Create S3 client\ns3_client = boto3.client(\n    's3',\n    endpoint_url=S3_ENDPOINT,\n    aws_access_key_id=S3_ACCESS_KEY,\n    aws_secret_access_key=S3_SECRET_KEY,\n    verify=False,  # For self-signed certs\n    config=config\n)\n\n# Create download directory\nPath(DOWNLOAD_PATH).mkdir(parents=True, exist_ok=True)\n\n# List and download all objects\npaginator = s3_client.get_paginator('list_objects_v2')\ndownloaded_count = 0\ndownloaded_files = []\n\nprint(\"Listing files...\")\nfor page in paginator.paginate(Bucket=S3_BUCKET, Prefix=S3_PREFIX):\n    if 'Contents' not in page:\n        print(\"No contents found!\")\n        continue\n\n    for obj in page['Contents']:\n        s3_key = obj['Key']\n\n        # Skip directory markers\n        if s3_key.endswith('/'):\n            continue\n\n        # Calculate local path\n        relative_path = s3_key[len(S3_PREFIX):] if S3_PREFIX else s3_key\n        local_file = os.path.join(DOWNLOAD_PATH, relative_path)\n\n        # Create local directory\n        Path(local_file).parent.mkdir(parents=True, exist_ok=True)\n\n        # Download file\n        print(f\"  [{downloaded_count+1}] {s3_key}\")\n        s3_client.download_file(S3_BUCKET, s3_key, local_file)\n        downloaded_count += 1\n        downloaded_files.append(local_file)\n\nprint(f\"\\nDownloaded {downloaded_count} files to {DOWNLOAD_PATH}\")\nprint(f\"First 5 files: {downloaded_files[:5]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Discover Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = []\n",
    "for root, _, filenames in os.walk(DOWNLOAD_PATH):\n",
    "    for filename in filenames:\n",
    "        if any(filename.endswith(ext) for ext in FILE_EXTENSIONS):\n",
    "            full_path = os.path.join(root, filename)\n",
    "            files.append(full_path)\n",
    "\n",
    "print(f\"Discovered {len(files)} files\")\n",
    "print(f\"\\nFirst 10 files:\")\n",
    "for f in files[:10]:\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Save for next step\n",
    "discovered_files = files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Single Document Ingestion\n",
    "\n",
    "Let's test ingesting just ONE document first to see if the API works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "if not discovered_files:\n",
    "    print(\"No files discovered! Run Step 2 first.\")\n",
    "else:\n",
    "    # Test with first file\n",
    "    test_file = discovered_files[0]\n",
    "    print(f\"Testing ingestion with: {test_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Read file content\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            file_content = f.read()\n",
    "        \n",
    "        print(f\"File size: {len(file_content)} characters\")\n",
    "        print(f\"First 200 chars: {file_content[:200]}...\")\n",
    "        \n",
    "        # Prepare payload\n",
    "        filename = Path(test_file).name\n",
    "        payload = {\n",
    "            \"content\": file_content,\n",
    "            \"metadata\": {\n",
    "                \"source\": \"jupyter-test\",\n",
    "                \"file_path\": test_file,\n",
    "                \"filename\": filename\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nSending to: {SERVICE_URL}/api/v1/collections/{COLLECTION_NAME}/documents\")\n",
    "        \n",
    "        # Send request\n",
    "        response = requests.post(\n",
    "            f\"{SERVICE_URL}/api/v1/collections/{COLLECTION_NAME}/documents\",\n",
    "            json=payload,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nResponse status: {response.status_code}\")\n",
    "        print(f\"Response headers: {dict(response.headers)}\")\n",
    "        print(f\"Response body: {response.text}\")\n",
    "        \n",
    "        if response.status_code in [200, 201]:\n",
    "            result = response.json()\n",
    "            doc_id = result.get('document_id', 'unknown')\n",
    "            print(f\"\\nSUCCESS: Document ID {doc_id}\")\n",
    "        else:\n",
    "            print(f\"\\nFAILED: HTTP {response.status_code}\")\n",
    "            print(f\"Error: {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Batch Ingestion (All Documents)\n",
    "\n",
    "Only run this after Step 3 works successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if not discovered_files:\n",
    "    print(\"No files discovered! Run Step 2 first.\")\n",
    "else:\n",
    "    print(f\"Processing {len(discovered_files)} files in batches of {BATCH_SIZE}\")\n",
    "    print(f\"Target collection: {COLLECTION_NAME}\")\n",
    "    \n",
    "    batch_results = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(discovered_files), BATCH_SIZE):\n",
    "        batch = discovered_files[i:i + BATCH_SIZE]\n",
    "        print(f\"\\nProcessing batch {i//BATCH_SIZE + 1}: {len(batch)} files\")\n",
    "        \n",
    "        for file_path in batch:\n",
    "            try:\n",
    "                # Read file content\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    file_content = f.read()\n",
    "                \n",
    "                # Prepare request\n",
    "                filename = Path(file_path).name\n",
    "                payload = {\n",
    "                    \"content\": file_content,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": \"jupyter-batch\",\n",
    "                        \"file_path\": file_path,\n",
    "                        \"filename\": filename\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Send request\n",
    "                response = requests.post(\n",
    "                    f\"{SERVICE_URL}/api/v1/collections/{COLLECTION_NAME}/documents\",\n",
    "                    json=payload,\n",
    "                    headers={\"Content-Type\": \"application/json\"},\n",
    "                    timeout=300\n",
    "                )\n",
    "                \n",
    "                if response.status_code in [200, 201]:\n",
    "                    result = response.json()\n",
    "                    doc_id = result.get('document_id', 'unknown')\n",
    "                    print(f\"  SUCCESS: {filename}: Document ID {doc_id}\")\n",
    "                    batch_results.append({\n",
    "                        \"file\": file_path,\n",
    "                        \"success\": True,\n",
    "                        \"document_id\": doc_id\n",
    "                    })\n",
    "                    successful += 1\n",
    "                else:\n",
    "                    error_detail = response.text\n",
    "                    print(f\"  FAILED: {filename}: HTTP {response.status_code} - {error_detail}\")\n",
    "                    batch_results.append({\n",
    "                        \"file\": file_path,\n",
    "                        \"success\": False,\n",
    "                        \"error\": f\"HTTP {response.status_code}: {error_detail}\"\n",
    "                    })\n",
    "                    failed += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: {file_path}: {str(e)}\")\n",
    "                batch_results.append({\n",
    "                    \"file\": file_path,\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "                failed += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Summary: {successful}/{len(discovered_files)} files ingested successfully\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Show failed files if any\n",
    "    if failed > 0:\n",
    "        print(\"\\nFailed files:\")\n",
    "        for result in batch_results:\n",
    "            if not result[\"success\"]:\n",
    "                print(f\"  {result['file']}: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Ingestion in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "if not DB_PASSWORD:\n",
    "    print(\"ERROR: DB_PASSWORD not set! Update configuration cell.\")\n",
    "else:\n",
    "    print(f\"Connecting to database: {DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        database=DB_NAME\n",
    "    )\n",
    "    \n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Query document statistics\n",
    "    print(\"\\nQuerying documents table...\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT\n",
    "            COUNT(*) as total_documents,\n",
    "            COUNT(DISTINCT collection_id) as total_collections\n",
    "        FROM documents\n",
    "    \"\"\")\n",
    "    doc_stats = cur.fetchone()\n",
    "    \n",
    "    # Query embedding statistics\n",
    "    print(\"Querying embeddings table...\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT COUNT(*) as total_embeddings\n",
    "        FROM embeddings\n",
    "    \"\"\")\n",
    "    emb_stats = cur.fetchone()\n",
    "    \n",
    "    # Query collection info\n",
    "    print(\"Querying collections table...\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, name, description, created_at\n",
    "        FROM collections\n",
    "    \"\"\")\n",
    "    collections = cur.fetchall()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Database Statistics:\")\n",
    "    print(f\"  Total documents: {doc_stats[0]}\")\n",
    "    print(f\"  Total collections: {doc_stats[1]}\")\n",
    "    print(f\"  Total embeddings: {emb_stats[0]}\")\n",
    "    print(f\"\\nCollections:\")\n",
    "    for col in collections:\n",
    "        print(f\"  - {col[1]} (ID: {col[0]}): {col[2]}\")\n",
    "        print(f\"    Created: {col[3]}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Sample some documents\n",
    "    print(\"\\nSample documents (first 5):\")\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, collection_id, content_preview, created_at\n",
    "        FROM documents\n",
    "        ORDER BY created_at DESC\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    docs = cur.fetchall()\n",
    "    for doc in docs:\n",
    "        print(f\"\\n  Document ID: {doc[0]}\")\n",
    "        print(f\"  Collection ID: {doc[1]}\")\n",
    "        print(f\"  Preview: {doc[2][:100]}...\")\n",
    "        print(f\"  Created: {doc[3]}\")\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\nDatabase connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Search\n",
    "\n",
    "Verify we can search the ingested documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test search query\n",
    "search_query = \"troubleshooting\"\n",
    "limit = 5\n",
    "\n",
    "print(f\"Searching for: '{search_query}'\")\n",
    "print(f\"Limit: {limit} results\")\n",
    "\n",
    "payload = {\n",
    "    \"query\": search_query,\n",
    "    \"limit\": limit\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{SERVICE_URL}/api/v1/collections/{COLLECTION_NAME}/search\",\n",
    "    json=payload,\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "print(f\"\\nResponse status: {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    results = response.json()\n",
    "    print(f\"\\nFound {len(results)} results:\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"  Document ID: {result.get('document_id', 'N/A')}\")\n",
    "        print(f\"  Score: {result.get('score', 'N/A')}\")\n",
    "        print(f\"  Preview: {result.get('content', 'N/A')[:200]}...\")\n",
    "        if 'metadata' in result:\n",
    "            print(f\"  Metadata: {result['metadata']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "else:\n",
    "    print(f\"Search failed: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Cell\n",
    "\n",
    "Use this cell for ad-hoc debugging queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any debugging code here\n",
    "# For example, check if collection exists:\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(f\"{SERVICE_URL}/api/v1/collections\")\n",
    "print(f\"Collections: {response.json()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}